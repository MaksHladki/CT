<!doctype html>
<html lang="ru">

<head>
	<meta charset="utf-8">
	<title>Название лекции</title>
	<meta name="description" content="">
	<meta name="keywords" content="">
	<meta name="author" content="Maks Hladki">
	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
	<link rel="stylesheet" href="../../css/bundle.min.css">
	<!--[if lt IE 9]>
		<script src="../../js/html5shiv.min.js"></script>
	<![endif]-->
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<section>
					<h1>Название лекции</h1>
					<h3>Разработка динамичесих веб приложений</h3>
					<p>
						<small>Гладкий Максим Валерьевич / <a href="https://github.com/MaksHladki">github:MaksHladki</a></small>
					</p>
				</section>
				<section>
					<h2>Содержание лекции</h2>
					<nav id="presentable-toc" class="table-content"></nav>
				</section>
			</section>
			<section>
				<blockquote>
					Это облачное хранилище, в котором пользователи хранят свои файлы. У нас 500 миллионов пользователей, у нас более 200 тысяч
					бизнесов, а также огромное количества данных и трафика (более 1.2 млрд новых файлов в день).
				</blockquote>
				<blockquote>
					Dropbox — файловый хостинг компании Dropbox Inc., включающий персональное облачное хранилище, синхронизацию файлов и программу-клиент.
					Штаб-квартира компании расположена в Сан-Франциско.
				</blockquote>
				<blockquote>
					Dropbox позволяет пользователям создать специальную папку на своих компьютерах, которую Dropbox синхронизирует таким образом,
					что она имеет одинаковое содержимое независимо от того, какое устройство используется для просмотра[2]. Файлы, размещённые
					в этой папке, также доступны через веб-сайт Dropbox и мобильные приложения. Dropbox работает по модели Freemium, в которой
					пользователи имеют возможность создать бесплатный аккаунт с заданным количеством свободного пространства, в то время
					как для увеличения объёма аккаунта необходима платная подписка[3].
				</blockquote>
			</section>
			<section>
				<h3>Особенности</h3>
				<blockquote>
					Dropbox позволяет пользователю размещать файлы на удалённых серверах при помощи клиента или с использованием веб-интерфейса
					через браузер[5][6]. При установке клиентского программного обеспечения Dropbox на компьютере создаётся синхронизируемая
					папка. Хотя главный акцент технологии делается на синхронизацию и обмен информацией, Dropbox ведёт историю загрузок,
					чтобы после удаления файлов с сервера была возможность восстановить данные[7][8]. Также ведётся история изменения файлов,
					которая доступна на период последних 30 дней[9], помимо этого доступна функция бессрочной истории изменения файлов «Pack-Rat»[10].
				</blockquote>
				<blockquote>
					История изменения файлов ведётся по принципу diff-кодирования, чтобы сэкономить место, занимаемое файлами. В истории изменения
					записывается только отличие одной версии файла от другой[11]. Файлы, загруженные через клиент, не имеют ограничения
					на размер, но файлы, загруженные через веб-интерфейс, ограничены 20 ГБ[12]. Есть также возможность выкладывать файлы
					для общего доступа через папку «Public», что позволяет использовать сервис в качестве файлообменника. В версиях 0.8.x
					также появилась возможность предоставления в общий доступ любой папки в «My Dropbox» для последующего доступа через
					так называемый «shareable link», то есть через веб-интерфейс. Для совместной работы над проектами сервис имеет возможность
					создания «Shared» папок для общего доступа лиц, имеющих разные учётные записи на сервисе. Доступна автоматическая синхронизация
					файлов и папок и хранение версий с возможностью отката[13].
				</blockquote>
				<blockquote>
					В отличие от ряда аналогов, Dropbox не использует шифрование данных на стороне клиента, что, в частности, сделало возможным
					инцидент 19 июня 2011 года, когда из-за ошибки в обновлённом программном обеспечении сервера в течение четырёх часов
					был возможен вход в любой аккаунт с использованием любого пароля[14][15]
				</blockquote>
				<blockquote>
					Сервис предлагает бесплатно 2 ГБ для хранения данных, которые можно увеличить бесплатно до 16 ГБ, приглашая новых пользователей
					или же получить несколько гигабайт после выполнения заданий (установка приложения Dropbox на мобильный телефон и т.
					д.). А также можно купить 1 ТБ.
				</blockquote>
				<ul>
					<li>Хранить файлы в безопасном месте</li>
					<li>Делиться файлами с другими людьми</li>
					<li>Постоянно иметь к ним доступ вне зависимости от своего месторасположения</li>
				</ul>
			</section>
			<section>
				<h2>Язык разработки</h2>
				<blockquote>
					Dropbox клиент разработан в основном на языке Python с использованием сторонних библиотек, таких как librsync. Клиент поддерживает
					все основные ОС: Windows, Mac, Linux. Использование Python однозначно говорит о том, что клиент разрабатывался с учетом
					облегченного портирования на различные платформы.
				</blockquote>
			</section>
			<section>
				<h2>Взрыной рост</h2>
				<ul>
					<li>1 миллион файлов сохраняются в Dropbox каждые 15 минут (по презентации это больше, чем твитов в Twitter за тот же период времени, но это несколько преувеличено)</li>
					<li>Одно из самых скачиваемых приложений, уступает лишь Skype</li>
					<li>Важная часть жизни многих пользователей: "не могу жить без этого"</li>
					<li>рост обеспечен "сарафанным радио", практически без рекламы</li>
				</ul>
			</section>
			<section>
				<h2>Архитектура</h2>
				<ul>
					<li>Первый кусок — это сервер метаданных, в нем хранится информация о файлах, связи между файлами, информация о пользователях,
						какая-то бизнес логика, и все это связано с базой данных.</li>
					<li>Второй большой кусок — это блочный storage в котором хранятся данные пользователей. Изначально, в 2011 году, все данные
						хранились в Amazon S3. В 2015 году, когда нам удалось перекачать все эксабайты к себе, мы рассказали о том, что написали
						свой облачный storage. ( Magic Pocket)</li>
				</ul>
				<blockquote>
					Мы назвали его Magic Pocket. Он написан на Go, частично на Rust и достаточно много на Python. Архитектура Magic Pocket, он
					кросс-зоный, есть несколько зон. В Америке это три зоны, они объединены в Pocket. Есть Pocket в Европе, с американским
					он не пересекается, потому что жители Европы не хотят, чтобы их данные были в Америке. Зоны между собой реплицируют
					данные. Внутри зоны есть ячейки. Есть Master, который управляет этими ячейками. Есть репликации между зонами. В каждой
					ячейке есть Volume Manager, который следит за серверами, на которых хранятся эти данные, там достаточно большие сервера.
				</blockquote>
				<blockquote>
					На каждом из серверов это все объединено в bucket, bucket – это 1 GB. Мы оперируем bucket’ами, когда перекидываем данные
					куда-то, когда что-то удаляем, очищаем, дефрагментируем, потому что сами блоки данных, которые мы сохраняем от пользователя,
					— это 4 MB, и оперировать ими очень сложно. Все компоненты Magic Pocket хорошо описаны в нашем техническом блоге, про
					них я не буду рассказывать.
				</blockquote>
			</section>
			<section>
				<h2>Как же мы на самом деле улучшаем доступность и сохранность данных?
				</h2>
				<p>Категории</p>
				<ul>
					<li>Изоляция;</li>
					<li>Защита;</li>
					<li>Контроль;</li>
					<li>Автоматизация.</li>
				</ul>
			</section>
			<section>
				<section>
					<h2>Виды изоляции</h2>
					<ul>
						<li>Физическая;</li>
						<li>Логическая;</li>
						<li>Эксплуатационная.</li>
					</ul>
				</section>
				<section>
					<h3>Физическая изоляция</h3>
					<blockquote>
						На масштабах Dropbox или компании вроде Dropbox нам очень важно общаться с дата центром, мы должны знать, как наши сервисы
						располагаются внутри дата центра, как к ним подводится энергия, какая сетевая доступность у этих сервисов. Мы не хотим
						в одной стойке держать сервисы баз данных, которые нам нужно постоянно бэкапить. Допустим, каждый бэкап у нас 400 Мбит/с,
						и у нас просто канала не хватит. Чем глубже вы уходите в этот стэк, тем дороже ваше решение, и тем оно становится сложнее.
						Насколько низко спускаться — это ваше решение, но, само собой, вы не должны класть все реплики ваших баз данных в одну
						стойку. Потому что энергия отключится и у вас баз данных нет больше.
					</blockquote>
					<blockquote>
						Можно посмотреть на все это в другом измерении, с точки зрения производителя оборудования. Очень важно пользоваться разными
						производителями оборудования, разными прошивками, разными драйверами. Почему? Хоть производители оборудования и говорят,
						что их решения надежны, на самом деле они врут и это не так. Хорошо хоть не взрываются.
					</blockquote>
					<blockquote>
						Исходя из всего этого, важно критические данные класть не только у себя где-то в бэкапах на своей инфраструктуре, но и во
						внешней инфраструктуре. Например, если вы у облака Google, то важные данные кладете в Amazon, и наоборот. Потому что
						если ваша инфраструктура погаснет, то бэкап будет взять неоткуда.
					</blockquote>
				</section>
				<section>
					<h2>Логическая изоляция</h2>
					<blockquote>
						Про нее практически все всё знают. Основные проблемы: если один сервис начинает создавать какие-то проблемы, то другие сервисы
						тоже начинают испытывать проблемы. Если баг был в коде у одного сервиса, то этот баг начинает распространяться на другие
						сервисы. У вас начинают поступать неправильные данные. Как с этим справиться?
					</blockquote>
					<blockquote>
						Слабая связанность! Но это очень редко работает. Есть такие системы, которые не слабо связаны. Это базы данных, ZooKeeper.
						Если у вас большая нагрузка пошла на ZooKeeper, у вас кластер кворум упал, то он весь упал. С базами данными примерно
						то же самое. Если большая нагрузка на master, то скорее всего весь кластер упадет.
					</blockquote>
					<img src="img/zone.png" alt="">
					<blockquote>
						Это высокоуровневая диаграмма нашей архитектуры. У нас есть две зоны, и между ними мы сделали очень простой интерфейс. Это
						практически put и get, это именно для storage. Это было очень сложно для нас, потому что мы хотели сделать все сложнее.
						Но это очень важно, потому что внутри зон все очень сложно, там и ZooKeeper, и базы данных, кворумы. И это все периодически
						падает, прям все сразу. И чтобы это не захватило остальные зоны, между ними есть этот простой интерфейс. Когда одна
						зона падает, то вторая скорее всего будет работать.
					</blockquote>
				</section>
				<section>
					<h3>Эксплуатационная изоляция</h3>
					<blockquote>
						Как бы вы хорошо ни раскидали ваш код по разным серверам, как бы хорошо его логически ни изолировали, всегда найдется человек,
						который сделает что-то не так. Например, в «Одноклассниках» была проблема: человек раскатал на все сервера Bash shell,
						в котором что-то не работало, и все сервера отключились. Такие проблемы тоже бывают.
					</blockquote>
					<blockquote>
						Еще шутят, что если бы все программисты и системные администраторы ушли куда-нибудь отдыхать, то система бы работала гораздо
						стабильнее, чем когда они работают. И это действительно так. Во время фризов многие компании имеют такую практику делать
						заморозку перед новогодними каникулами, система работает гораздо стабильнее.
					</blockquote>
					<blockquote>
						Контроль доступа. Релиз процесс: все это активно тестируете, затем тестируете на staging, который использует, например, ваша
						компания. Дальше мы выкладываем изменения в одну зону. Как только мы удостоверились, что все нормально, мы раскладываем
						на остальные две зоны. Если что-то не нормально, то мы с них реплицируем данные. Это все касается storage. Продуктовые
						сервисы мы постоянно обновляем, раз в день.
					</blockquote>
				</section>
				<section>
					<section>
						<h2>Защита</h2>
						<blockquote>
							Это валидация операций. Это возможность восстановить эти данные. Это тестирование. Что такое валидация операций?
						</blockquote>
					</section>
				</section>
				<section>
					<section>
						<h2>Валидация операций</h2>
						<blockquote>
							Самый большой риск для системы — это оператор.
						</blockquote>
						<blockquote>
							Мы поменяли синтаксис команды (gsh), чтобы не было больше таких проблем. Мы запретили выполнять деструктивные операции на
							живых сервисах (DB, memcache, storage). То есть нам нельзя ни к коем случае перезагрузить базу данных, не остановив
							её и не убрав её из production, так же с memcache. Все такие операции мы стараемся автоматизировать.
						</blockquote>
					</section>
					<section>
						<h2>Второй пример</h2>
						<blockquote>
							Это SQLAlchemy. Это библиотека для Python для работы с базами данных. И в ней для update, insert, delete есть такой аргумент,
							который называется whereclause. В нем вы можете указать что вы хотите удалить, что вы хотите апдейтить. Но если вы
							туда передадите не whereclause, а where, то sqlalchemy ничего не скажет, он просто удалит все без where, это очень
							большая проблема. У нас есть несколько сервисов, например, ProxySQL. Это прокси для MySQL, который позволяет запретить
							многие деструктивные операции (DROP TABLE, ALTER, updates без where и т.д.). Так же в этом ProxySQL можно сделать
							throttling и для тех запросов, которые мы не знаем, ограничить их количество, чтобы умный запрос не положил нам master
							случайно.
						</blockquote>
					</section>
				</section>
			</section>
			<section>
				<h2>Восстановление</h2>
				<blockquote>
					Восстановление. Очень важно не только создавать бэкапы, но и проверять, что эти бэкапы восстановятся. Facebook недавно выложил
					статью, где они рассказывают, как они постоянно делают бэкапы и постоянно восстанавливаются из этих бэкапов. У нас по
					сути все тоже самое. Вот пример из нашего Orchestrator, за какой-то короткий отрезок времени:
				</blockquote>
				<blockquote>
					Видно, что у нас постоянно делаются клоны баз данных, потому что у нас на одном сервере до 32 баз данных расположено, мы
					постоянно их перемещаем. Поэтому у нас постоянно происходит клонирование. Постоянно promotion идут в master и на slave
					и т.д. Так же огромное количество бэкапов. Мы бэкапимся к себе, также в Amazon S3. Но у нас также постоянно происходит
					recovery. Если мы не проверим, что каждая база данных, которую мы забэкапили, может восстановиться, то по сути у нас
					этого бэкапа нет.
				</blockquote>
			</section>
			<section>
				<section>
					<h3>Тестирование</h3>
					<blockquote>
						Все знают, что полезно и юнит-тестирование, и интеграционное тестирование. С точки зрения доступности, тестирование — это
						MTTR, время для восстановления, оно по сути равно 0. Потому что вы этот баг нашли не в production, а до production
						и пофиксили его. Availability не прогнулось. Это тоже очень важно.
					</blockquote>
				</section>
			</section>
			<section>
				<h2>Контроль</h2>
				<blockquote>
					то-то всегда напортачит: либо программисты, либо операторы что-то сделают не так. Это не проблема. Нужно иметь возможность
					находить и исправлять это всё. У нас для storage существует огромное количество верификаторов.
				</blockquote>
				<img src="img/verification.png" alt="">
				<blockquote>
					Их на самом деле 8, а не 5, как здесь. У нас коды верификаторов больше кодов самого storage. У нас 25% внутреннего трафика
					— это верификации. На самом низком уровне работают disk scrubber’ы, которые просто читают блоки с жесткого диска и проверяют
					контрольные суммы. Почему мы это делаем? Потому что жесткие диски врут, S.M.A.R.T. врут. Производителям невыгодно, чтобы
					S.M.A.R.T находил ошибки, потому что им приходится возвращать эти жесткие диски. Поэтому это нужно всегда проверять.
					И как только мы видим проблему, мы пытаемся восстановить эти данные.
				</blockquote>
				<blockquote>
					У нас есть trash inspector. Когда мы что-то удаляем, либо перемещаем, либо что-то деструтивное делаем с данными, мы сначала
					кладём эти данные в некую корзину и потом проверяем эти данные, действительно ли мы хотели их удалить. Хранятся они
					там две недели, например. У нас capacity на это двухнедельное время удаленных данных, это очень важно, поэтому мы тратим
					на это деньги. Мы так же постоянно часть трафика, который приходит в storage, сохраняем эти операции в Kafka. Потом
					эти операции повторяем уже на storage. Мы обращаемся к storage как к blackbox, чтобы посмотреть, действительно ли там
					есть данные трафика, который к нам пришел, и те, которые записались, мы их можем забрать.
				</blockquote>
			</section>
			<section>
				<section>
					<h2>Автоматизация</h2>
				</section>
				<section>
					<blockquote>
						Самая важная вещь. Когда у вас количество серверов растет линейно или экспоненциально, то количество людей не рождается линейно.
						Они рождаются, учатся, но с какой-то периодичностью. И вы не можете увеличивать количество людей равносильно количеству
						ваших серверов. Поэтому вам нужна автоматизация, которая будет выполнять работу за этих людей.
					</blockquote>
				</section>
				<section>
					<blockquote>
						В автоматизации очень важно собирать метрики с вашей инфраструктуры. Я практически ничего не говорил про метрики в своем
						докладе, потому что метрики — это ядро вашего сервиса и про него упоминать не надо, потому что это самая важная часть
						вашего сервиса. Если у вас нет метрик, вы не знаете, работает ваш сервис или нет. Поэтому очень важно собирать метрики
						быстро. Если, например, у вас метрики собираются раз в минуту, а проблема у вас в минуте, то вы про нее не узнаете.
						Также очень важно быстро реагировать. Если реагирует человек, например, у нас минута прошла, когда что-то случилось,
						что-то закладывается на то, что баг был в метрике, вам приходит alert. У нас policy в течение 5 минут. Вы должны начать
						что-то делать, реагировать на alert в это время. Вы начинаете что-то делать, начинаете разбираться, по сути у вас проблема
						решается в среднем минут 10-15, в зависимости от проблемы. Автоматизация позволяет вам ускорить, но не решить, в том
						плане, что она дает информацию об этой проблеме до того, как вы займетесь решением данной проблемы.
					</blockquote>
				</section>
				<section>
					<blockquote>
						У нас есть такой инструмент Naoru — параноидальная автоматизация.
					</blockquote>
					<img src="img/параноидальная_автоматизация.png" alt="">
				</section>
				<section>
					<h3>Структура параноидаольной автоматизации</h3>
					<blockquote>
						Она состоит из неких алертов, которые приходят. Это может быть простой пайтоновский скрипт, который коннектится к серверу
						и проверяет, что он доступен. Это может приходить из Nagios или Zabbix, неважно что вы используете. Главное, чтобы
						это приходило быстро. Дальше мы должны понять, что с этим делать, мы должны продиагностировать. Например, если сервер
						недоступен, мы должны попробовать подключится по SSH, подключится по IPMI, посмотреть, если нет ответа, он завис или
						еще что-то, вы прописываете какое-то лечение.
					</blockquote>
					<blockquote>
						Дальше, когда вы пишите автоматизацию, все должно пройти через оператора. У нас есть policy, что мы любую автоматизацию,
						примерно 3-6 месяцев она решается через оператора.
					</blockquote>
					<blockquote>
						Мы собрали всю информацию о проблеме, и оператору вываливается эта информация, такой-то сервер недоступен по такой-то причине,
						и указывается, что нужно сделать, и спрашивается подтверждение у оператора. Оператор обладает очень важными знаниями.
						Он знает, что сейчас какая-то проблема с сервисом. Он, например, знает, что этот сервер нельзя перезагружать, потому
						что на нем еще что-то запущено. Поэтому его нельзя просто перезапустить. Поэтому каждый раз, когда оператор сталкивается
						с какой-то проблемой, он вносит какие-то улучшения в этот скрипт автоматизации, и с каждым разом он становится все
						лучше.
					</blockquote>
				</section>
				<section>
					<h3>система Wheelhouse</h3>
					<img src="img/wheelhouse.png" alt="">
					<blockquote>
						По сути у нас есть кластер базы данных, у которой есть один master. A и есть два slave. Нам нужно заменить этот master, например,
						мы хотим обновить ядро. Чтобы его заменить нам нужно запромоутить slave, депромоутить тот master, удалить его. У нас
						есть требование в Dropbox, у нас в кластере всегда должно быть два slave для такой конфигурации. У нас есть некое состояние
						этого кластера. HostA находится в production, он master, он еще не освобожден, количество slave у нас два, но нам нужно
						три для этой операции.
					</blockquote>
					<blockquote>
						Из replace_loop (синяя стрелочка) мы видим, что наш сервер находится в production, и нам не хватает slave’ов, и мы переходим
						в состояние выделить еще slave. Мы приходим в это состояние, мы инициируем работу создать новый клон с master’a новый
						slave. Это запускается где-то в Orchestrator, мы ждем. Если работа завершена, и все хорошо, то мы переходим к следующему
						шагу. Если был fail, то переходим в состояние failure. Дальше мы добавляем в этот хост новый slave в production, тоже
						инициируем в Orchestrator эту работу, ждем и возвращаемся в replace_loop.
					</blockquote>
					<img src="img/wheelhouse_2.png" alt="">
					<blockquote>
						На самом деле это depromote. Потому что нам нужно master сделать slave’ом, а какой-то slave master’ом. Тут все тоже самое.
						Добавляется работа в Orchestrator, проверятся условие и так далее, остальные шаги примерно такие же. Мы после этого
						удаляем master с production, убираем с него трафик. Удаляем master в инсталлятор, чтобы люди, которые занимаются этим
						сервером, могли что-то обновить на нем.
					</blockquote>
					<blockquote>
						Этот блок очень маленький, но он участвует в более сложных блоках. Например, если нам нужно перенести целую стойку в другую
						стойку, потому что драйвера на свитчах меняются, и нам нужно просто всю стойку отключить чтобы перезагрузить свитч.
						У нас много описано таких диаграмм-состояний для разных сервисов, как их гасить, как их поднимать. Это все работает.
						Даже математически можно доказать, что у вас система всегда будет находиться в рабочем состоянии. Почему это сделано
						через STM, а не в режиме процедур? Потому что если это долгий процесс, например, клонирование может занимать час или
						около того, то у вас может случиться что-то еще, и состояние вашей системы изменится. В случае с машиной состояний
						вы всегда знаете, какое состояние и как на него реагиров
					</blockquote>
				</section>
			</section>
			<section>
				<h2>Блоки</h2>
				<blockquote>
					Основной элемент системы — это блок (chunk) размером до 4 Mb. В случае, если файл большего размера, он разбивается на несколько
					блоков, и каждый блок воспринимается системой независимо от других. Для каждого блока вычисляется SHA256 хеш, и эта
					информация является частью метаинформации о файле. Dropbox уменьшает объем передаваемых данных за счет передачи только
					разницы между измененными блоками файла. Кроме того, локально он содержит всю метаинформацию по файлам, которую синхронизирует
					с сервером и передает только изменения с прошлой версии (incremental updates).
				</blockquote>
				<blockquote>
					Dropbox использует два типа серверов: управляющий (control) и сервер данных (data storage). Сервера управления находятся
					под контролем Dropbox, сервера данных — это сервера Амазона (Amazon S3, EC2). Для коммуникациями с серверами во всех
					случаях используется HTTPS.
				</blockquote>
			</section>
			<section>
				<h2>Доменные имена</h2>
				<table>
					<thead>
						<tr>
							<th>Поддомен</th>
							<th>Хостинг</th>
							<th>Описание</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>client-lb/clientX</td>
							<td>Dropbox</td>
							<td>Meta data</td>
						</tr>
						<tr>
							<td>notifyX</td>
							<td>Dropbox</td>
							<td>Notifications</td>
						</tr>
						<tr>
							<td>api</td>
							<td>Dropbox</td>
							<td>API control</td>
						</tr>
						<tr>
							<td>www</td>
							<td>Dropbox</td>
							<td>Web servers</td>
						</tr>
						<tr>
							<td>d</td>
							<td>Dropbox</td>
							<td>Event logs</td>
						</tr>
					</tbody>
				</table>
			</section>
			<section>
				<h2>Продолжение</h2>
				<table>
					<thead>
						<tr>
							<th>Поддомен</th>
							<th>Хостинг</th>
							<th>Описание</th>
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>dl</td>
							<td>Amazon</td>
							<td>Direct links</td>
						</tr>
						<tr>
							<td>dl-clientX</td>
							<td>Amazon</td>
							<td>Client storage</td>
						</tr>
						<tr>
							<td>dl-debugX</td>
							<td>Amazon</td>
							<td>Back traces</td>
						</tr>
						<tr>
							<td>dl-web</td>
							<td>Amazon</td>
							<td>Web storage</td>
						</tr>
						<tr>
							<td>api-content </td>
							<td>Amazon</td>
							<td>API storage</td>
						</tr>
					</tbody>
				</table>
			</section>
			<section>
				<section>
					<h2>протокол, используемый Dropbox для загрузки локально измененных блоков на свои сервера</h2>
				</section>
				<section>
					<h3>Схема</h3>
					<img src="img/upload_block.png" alt="">
				</section>
				<section>
					<h3>Описание</h3>
					<ul>
						<li>После регистрации клиента на управляющих серверах clientX.dropbox.com, команда list получает изменения в метаданных,
							которые показывают разницу между локальной копией и тем, что находится на сервере. </li>
						<li>Как только происходит локальное изменение файлов, Dropbox вызывает команду commit_batch (client-lb.dropbox.com) и посылает
							измененные метаданные на сервер. </li>
						<li>После этого сервер отвечает, какие блоки ему необходимы, используя команду need_blocks, и клиент отсылает эти блоки
							на Amazon (dl-clientX.dropbox.com). </li>
						<li>Сохранение каждого блока подтверждается командой ОК.</li>
						<li>После этого локальный клиент еще раз раз посылает команду commit_batch на сервер и получает подтверждение, что все
							блоки получены. Транзакции сохранения данных могут выполняться параллельно.</li>
					</ul>
				</section>
			</section>
			<section>
				<h2>Протокол управления</h2>
				<p>Dropbox использует следующие группы управляющих серверов:
				</p>
				<ul>
					<li>Уведомления (notifications). Dropbox держит постоянное открытое TCP соеденинение с серверами уведомлений (notifyX.dropbox.com).
						Это необходимо для получения информации об изменении файлов, которое могло произойти на других клиентах. По сравнению
						с другим трафиком, эта информация не шифруется. Используется задержка HTTP ответа для быстрого уведомления клиентов
						(push mechanism). Клиент посылает запрос, и сервер задерживает ответ примерно на 60 секунд. По истечении 60 секунд,
						клиент немедленно посылает следующий запрос на сервер. Если ответ сформирован раньше, то сервер отвечает немедленно.</li>
					<li>Управление метаданными (meta-data administration) Сервера управления метаданными отвечают не только за информирование
						об изменениях в блоках и файлах, но также и за авторизацию (authentication) клиента. Для этих серверов используются
						следующие доменные имена: client-lb.dropbox.com, clientX.dropbox.com. Кроме этого, сервера управления могут контролировать
						поведение клиента. В момент эксперимента было замечено, что сервера могут указать клиенту максимальное количество блоков,
						которое он может посылать на сервер. Это используется для управления трафиком, который генерирует клиент.</li>
					<li>Системные сообщения (system logs) сервера предоставляются Амазоном и имеют название dl-debug.dropbox.com; остальные
						сообщения идут непосредственно на Dropbox d.dropbox.com.</li>
				</ul>
			</section>
			<section>
				<section>
					<h3>Dropbox Infinite</h3>
				</section>
				<section>
					<blockquote>
						Ровно месяц назад Dropbox анонсировал Dropbox Infinite — «революционно новый способ доступа к вашим файлам», как писала компания
						в корпоративном блоге. В демонстрационном видео показали, что десктопный клиент Dropbox предоставляет прямой доступ
						к облачному хранилищу файлов на уровне файловой системы, без необходимости запускать браузер. Локальный диск «увеличивается»
						на размер облачного хранилища, файлы доступны напрямую. Облачное хранилище может быть больше по размеру, чем локальный
						диск. Сейчас компания раскрыла технические подробности, как работает эта функция.
					</blockquote>
					<blockquote>
						«Традиционно Dropbox работал полностью в пространстве пользователя, как любая другая программа на вашей машине, — пишет разработчик
						компании Дэмьен Девиль (Damien DeVille). — С Dropbox Infinite мы углубляемся ещё глубже: в пространство ядра. С этой
						технологией клиент Dropbox меняет роль от пассивного наблюдателя, который смотрит, что происходит на локальном диске,
						на активную роль в вашей файловой системе. Мы почти два года работали над тем, чтобы соединиться кусочки паззла вместе,
						чтобы они работали прозрачно».
					</blockquote>
					<blockquote>
						Разработчик объясняет, что обычная схема FUSE не устраивает их с точки зрения производительности: каждая файловая операция
						обычно требует лишнего переключения контекста между пространством ядра и пространством пользователя, см. схему FUSE).
					</blockquote>
				</section>
				<section>
					<h3>Схема FUSE</h3>
					<img src="img/FUSE.png" alt="">
				</section>
				<section>
					<blockquote>
						Производительность — не единственная причина. Dropbox считает, что замена стандартных библиотек FUSE на собственное расширение
						ядра устраняет излишнюю сложность и, следовательно, повышает безопасность системы.
					</blockquote>
					<blockquote>
						Ещё одна полезная вещь: в расширении ядра работает проверка прав доступа через Kernel Authorization (Copy Hooks в Windows),
						чтобы детектировать и запрещать определённые операции в папке Dropbox.
					</blockquote>
				</section>
				<section>
					<img src="img/infinity.png" alt="">
				</section>
				<section>
					<h3></h3>
					<blockquote>
						Если приложение работает в пространстве ядра, оно может позволить себе гораздо больше, чем обычная программа в пространстве
						пользователя. С точки зрения безопасности это довольно рискованно. «Если Dropbox в ядре, то может получить доступ к
						чему угодно, — говорит Сэм Боун (Sam Bowne), который ведёт курсы этического хакинга в колледже Сан-Франциско. — Если
						в клиенте Dropbox есть баг, его можно использовать для захвата всей системы».
					</blockquote>
				</section>
			</section>
			<section>
				<section>
					<h2>Фейлы</h2>
				</section>
				<section>
					<h3>2017</h3>
					<blockquote>
						На прошлой неделе на форуме Dropbox развернулось бурное обсуждение, в ходе которого многие пользователи сообщили, что они
						обнаружили в своих папках файлы, которые были удалены несколько лет назад. Некоторые утверждают, что вновь увидели
						файлы, с момента удаления которых прошло 7 лет.
					</blockquote>
					<blockquote>
						Сразу же было высказано предположение о том, что серверы и учетные записи Dropbox были взломаны. Однако представители сервиса
						успокоили пользователей, сообщив, что причина заключается в некоторой ошибке, из-за которой удаленные файлы действительно
						хранились на серверах Dropbox. В Dropbox не сообщили, какой объем дополнительного пространства был занят из-за этой
						ошибки, отметив лишь, что она полностью устранена.
					</blockquote>
				</section>
				<section>
					<h2>2012</h2>
					<blockquote>
						Спустя 4 года после взлома, руководство Dropbox признало факт утечки паролей. В руках хакеров оказались 68 млн паролей –
						почти 2/3 клиентов сервиса.
					</blockquote>
					<blockquote>
						В последнее время Dropbox преследуют неприятности. Неделю назад заголовки пестрили сообщениями и опровержениями о произошедшей
						в облачном сервисе утечке пользовательских паролей невиданного масштаба. В то время как данный взлом без сомнения представляет
						угрозу для пользователей сервиса Dropbox, совсем новым его назвать нельзя. Своими корнями он уходит в 2012 год, когда
						сервис сообщил о крупной утечке адресов электронной почты своих пользователей. Интересно, что в сообщениях об инциденте
						трехлетней давности не было ни одного упоминания об похищенных паролях. Компания утверждала, что украдены были только
						почтовые адреса. Только теперь становится ясно, что прошлый инцидент не был исчерпан в 2012 году. Сразу после сообщения
						об инциденте с утечкой более 68 млн паролей, в официальном блоге компании Dropbox было опубликовано сообщение с ценными
						рекомендациями для пользователей. Ниже приведены некоторые из тех советов и рекомендаций, которые Dropbox дает своим
						пользователям:
					</blockquote>
				</section>
			</section>
			<!-- Содержимое лекции -->
			<section>
				<h2 class="header-hide">Спасибо за внимание</h2>
				<img src="img/thanks.jpg" alt="" height="600">
			</section>
		</div>
	</div>
	<aside id="presentable-icon" class="revealjs">
		<a title="Содержание лекции" href="#/0/1">
			<i class="fa fa-list-ul fa-2x"></i>
		</a>
	</aside>
	<script src="../../js/bundle.min.js"></script>
</body>

</html>